---
title: "model_1"
author: "Sam Temlock"
date: "13/11/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lmtest)
library(sandwich)
library(tidyverse)
library(magrittr)

df <- read.csv("covid-19.csv", header = TRUE)
```

For the first model, the relationship between the COVID case rate per 100,000 and the population density per square mile of states was analyzed. First, the distribution of population density per square mile variable is examined.

```{r}
histogram_of_pdensity <- df %>%
  ggplot(aes(x = Population.density.per.square.miles)) + 
  geom_histogram(bins = 20) + 
  labs(
    title = 'Figure 1. Distribution of Population Density per Square Mile', 
    x = 'Population density per square mile', y = 'Count')

histogram_of_pdensity
```

As can be seen from Figure 1, although most of the population density is concentrated in the 0 to 1500 range, there are some grouping of outliers that are very far from this concentration. When an analysis is performed, it can be seen that there is only one data sample that is the outlier, which is the District of Columbia (D.C.). This is given due to the fact that D.C. is a district that solely consists of a large city, as mentioned as a possibility in the introduction. Given that this causes the data to be skewed, the logarithm is taken to scale the variable. Once this transformation is performed, it is shown that there is a relatively normal distribution of population densities (see Figure 2).

```{r}
# Find the outlier data points
outliers <- subset(df,Population.density.per.square.miles > 4000)
outliers

# Transform the variable by taking the logarithm and assign it to a new variable
df <- df %>% 
  mutate(l_population_density = log(Population.density.per.square.miles))

# Plot the new distribution in a histogram
histogram_of_pdensity <- df %>%
  ggplot(aes(x = l_population_density)) + 
  geom_histogram(bins = 20) + 
  labs(
    title = 'Figure 2. Distribution of Population Density per Square Mile without D.C.', 
    x = 'Log of population density per square mile', y = 'Count')

histogram_of_pdensity
```

Next, the distribution for the case rate of the states is examined for normality.

```{r}
histogram_of_case_rate <- df %>%
  ggplot(aes(x = Case.Rate.per.100000)) + 
  geom_histogram(bins = 20) + 
  labs(
    title = 'Figure 3. Distribution of Case Rate per 100,000', 
    x = 'Case rate per 100,000', y = 'Count')

histogram_of_case_rate
```

As can be seen above in Figure 3, the distribution is fairly normal, and given that it has already been standardized as a rate across all states, there is no need to perform any transformations on this variable.

```{r variables}

df_new %>% 
  ggplot(aes(x = Case.Rate.per.100000, y = Population.density.per.square.miles)) + 
  geom_point() 

df_new %>% 
  ggplot(aes(x = Death.Rate.per.100000, y = Population.density.per.square.miles)) + 
  geom_point() 
```
```{r}
model_current <- lm(Case.Rate.per.100000 ~ Population.density.per.square.miles , data = df_new, na.action = na.omit)
coeftest(model_current, vcov = vcovHC)

model_alternative <- lm(Death.Rate.per.100000 ~ Population.density.per.square.miles , data = df_new, na.action = na.omit)
coeftest(model_alternative, vcov = vcovHC)
```

```{r}

df<-df%>%
  rename(case_rate_100k = 'Case.Rate.per.100000',
         population_density = 'Population.density.per.square.miles',
         white_pct = 'White...of.Cases',
         black_pct = 'Black...of.Cases',
         hispanic_pct = 'Hispanic...of.Cases',
         other_pct = 'Other...of.Cases',
         mask_public='Mandate.face.mask.use.by.all.individuals.in.public.spaces',
         mask_legal='No.legal.enforcement.of.face.mask.mandate') %>%
  select(case_rate_100k, population_density, white_pct, black_pct, hispanic_pct, 
         other_pct, mask_public, mask_legal)
head(df)
```


You will next build a set of models to investigate your research question, documenting your decisions.  Here are some things to keep in mind during your model building process:

1. *What do you want to measure*?  Make sure you identify one, or a few, variables that will allow you to derive conclusions relevant to your research question, and include those variables in all model specifications.
2. Is your modeling goal one of description or explanation? 
3. What [covariates](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics_synonyms) help you achieve your modeling goals?  What covariates are problematic, either due to *collinearity*, or because they are outcomes that will absorb some of a causal effect you want to measure?
4. What *transformations*, if any, should you apply to each variable?  These transformations might reveal linearities in scatterplots, make your results relevant, or help you meet model assumptions.
5. Are your choices supported by exploratory data analysis (*EDA*)?  You will likely start with some general EDA to *detect anomalies* (missing values, top-coded variables, etc.).  From then on, your EDA should be interspersed with your model building.  Use visual tools to *guide* your decisions.  You can also leverage statistical *tests* to help assess whether variables, or groups of variables, are improving model fit.

At the same time, it is important to remember that you are not trying to create one perfect model.  You will create several specifications, giving the reader a sense of how robust (or sensitive) your results are to modeling choices, and to show that you're not just cherry-picking the specification that leads to the largest effects.

At a minimum, you should include the following three specifications:

1. **Model 1**: One model with *only the key variables* you want to measure (possibly transformed, as determined by your EDA), and no other covariates (or perhaps one, or at most two, covariates if they are so crucial that it would be unreasonable to omit them)

```{r}
df %>% 
  ggplot(aes(x = case_rate_100k, y = log(population))) + 
  geom_point() 
```

